---
title: "Continuous Time Stochastic Processes"
date:  '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  pdf_document:
    number_sections: true
header-includes:  
  \usepackage{dcolumn}
  \usepackage{amsmath}
  \usepackage{tabularx}
  \usepackage{array}
---

```{r echo=FALSE}
library(knitr)
library(kableExtra)
```

# Preliminaries

## Types of Processes

A **right continuous** stochastic process.

There are three types of right continuous processes

-   **Normal**

-   **Absorption**

-   **Explosion**

The **jump times** are random variables

The **holding times** are random variables defined as

A **jump process**

Compute probabilities using **countable union**

A **counting process** is a stochastic process $\{N_t\}_{t\geq 0}$ satisfying

-   $N_0=0$

-   $\forall t \geq 0, N_t \in \mathbb{N}_0$

-   (Non-decreasing) If $0 \leq s \leq t$, $N_s \leq N_t$

-   (Counting) When $s<t$, $N_t-N_s$ equals the no. of events in $(s,t]$

-   (Right continuous) The process is piecewise constant and has upward jumps (single step) of size $1$, therefore

$$
N_{t^-} = \lim_{s \uparrow t} N_s
$$

A **counting process associated the sequence** $(J_n)_{n \in \mathbb{N}_0}$

## Properties of random variables

The **exponential random variable** has

$$
f_X(x)=\lambda e^{-\lambda x}
$$

and c.d.f.

$$
F_X(x)=1-e^{-\lambda x}
$$

with a nonnegative support.

It has expectation

$$
\mathbb{E}[X]=\dfrac{1}{\lambda}
$$

and variance

$$
\textrm{Var}[X]=\dfrac{1}{\lambda^2}
$$

The **memoryless property** of a random variable refers to the fact:

$$
\Pr(X>x+y \mid X >x) = \Pr(X>y)
$$

-   A continuous random variable is memoryless iff it is $\textrm{Exp}(\lambda)$

-   A discrete random variable is memoryless iff it is $\textrm{Geom}(p)$

The **sum of exponential** $\textrm{Exp}(\lambda)$ is a $\textrm{Gamma}(n,\lambda)$ distribution

$$
f_{J_n}(t)=\dfrac{\lambda^n}{\Gamma(n)} t^{n-1} e^{-\lambda t}, \qquad t>0
$$

The **convergence for infinite sum of exponential** has the following criteria

-   If $\sum \frac{1}{\lambda_i} < \infty$, then $\Pr(J_{\infty}<\infty)<1$

-   If $\sum \frac{1}{\lambda_i}=\infty$, then $\Pr(J_{\infty}=\infty)=1$

The **minimum of exponential** is

$$
H \sim \textrm{Exp}(\sum_{i=1}^n \lambda_i)
$$

and the probability of any of the $k$ variables being the minimum is

$$
\Pr(H=H_k)=\dfrac{\lambda_k}{\sum_{i=1}^n \lambda_i}
$$

The **Laplace Transform** of a random variable $X$ is given by

$$
\mathcal{L}_X(u) = \mathbb{E}[e^{-uX}]
$$

A list of transformations for common random variables:

-   (Poisson) $\exp(\lambda t [e^{-u}-1])$

-   (Exponential) $\dfrac{\lambda}{\lambda+u}$

The **characteristic function** of a random variable $X$ is given by

$$
\phi_X(t) = \mathbb{E}[e^{itX}]
$$

# Poisson Processes

## Definitions

A **Poisson process**, denoted $\{N_t\}_{t\geq 0}$, is a non-decreasing stochastic process with nonnegative values satisfying

-   $N_0=0$

-   The increments are independent, $0 \leq t_0 \leq t_1 \leq \ldots \leq t_n$, the random variables $N_{t_0}, N_{t_1}-N_{t_0}, \ldots, N_{t_n}-N_{t_{n-1}}$ are independent

-   The increments are stationary

$$
\Pr(N_t-N_s=k)=\Pr(N_{t-s}=k)
$$

-   There is a single arrival (only one arrives in a small interval), for all $t \geq 0$ and $\delta >0$, $\delta \to 0$

$$
\begin{aligned}
    \Pr(N_{t+\delta}-N_t=1) &= \lambda \delta + o(\delta) \\
    \Pr(N_{t+\delta}-N_t \geq 2) &= o(\delta) \\
    \Pr(N_{t+\delta}-N_t = 0) &= 1- \lambda \delta + o(\delta)
\end{aligned}
$$

This also ensures that a Poisson process is continuous in probability.

An **equivalent definition** replaces the last condition with the variable being Poisson with rate $N_t$

$$
  \Pr(N_t=k) = \dfrac{(\lambda t)^k}{k!} e^{-\lambda t}
$$

Another **equivalent definition** characterizes Poisson process $\{N_t\}_{t\geq 0}$ explicitly

-   Let $H_1, H_2, \ldots$ denote i.i.d. $\textrm{Exp}(\lambda)$ random variables

-   Let $J_0=0$ and $J_n = \sum_{i=1}^n H_i$

-   We define

$$
N_t = \sup \{n \in \mathbb{N}_0: J_n \leq t \}
$$

## Properties of Poisson Process

### Inter-arrival times

The inter-arrival times are **i.i.d.** $\textrm{Exp}(\lambda)$ random variables

### Time to $n^{th}$ event

The time to $n^{th}$ event is defined as

$$
J_n = \sum_{i=1}^n H_i
$$

which follows a $\textrm{Gamma}(n,\lambda)$ distribution

$$
f_{J_n}(t)=\dfrac{\lambda^n}{\Gamma(n)} t^{n-1} e^{-\lambda t}, \qquad t>0
$$

### Conditional distribution of arrival times

The conditional joint density of $(J_1, \ldots, J_n)$ is given by the order statistic

$$
f_{(J_1, \ldots, J_n)}(t_1, \ldots, t_n \mid N_t=n)=\begin{cases}
  \dfrac{n!}{t^n} & 0<t_1<\ldots<t_n \\
  0 & \textrm{otherwise}
\end{cases}
$$

The expectation of the $k^{th}$ value of $n$ uniformly distributed order statistics on $[0,t]$ is\
$$
\mathbb{E}[X_{(k)}]=\dfrac{tk}{n+1}=\mathbb{E}[J_k \mid N_t=n]
$$

## Extensions to Poisson Processes

### Superposition

Given $n$ *independent* Poisson processes $\{N_t^{(1)}\}_{t\geq 0}, \ldots, \{N_t^{(n)}\}_{t\geq 0}$, with respective rates $\lambda_1, \ldots, \lambda_n >0$,

$$
N_t = \sum_{i=1}^n N_t^{(i)}
$$

is also a Poisson process with rate $\lambda = \sum_{i=1}^n \lambda_i$.

This is called a **superposition of Poisson processes**.

### Thinning

Each arrival of a Poisson Process $\{N_t\}_{t\geq 0}$ is marked as a type $k$ event with probability $p_k$, for $k=1, \ldots, n$, where $\sum_{k=1}^n p_k=1$. Then let $N_t^{(k)}$ denote the number of type $k$ events up to time $t$ (in $[0, t]$). Then each $N_t^{(k)}$ is a Poisson process with rate $\lambda p_k$.

Each process is called a **thinned Poisson Process**.

## Non-homogenous Poisson processes

Let $\lambda: [0, \infty) \to (0, \infty)$ denote a non-ngative and locally integrable function. Then the process $N = \{N_t\}_{t\geq 0}$ is a **non-homogenous Poisson process** with intensity function $\lambda(t)$ if

-   $N_0=0$

-   $N$ has independent increments

-   Single arrival; for all $t \geq 0$ and $\delta >0$,

$$
\begin{aligned}
  \Pr(N_{t+\delta}-N_t=1) &= \lambda (t) \delta + o(\delta) \\
\Pr(N_{t+\delta}-N_t \geq 2) &= o(\delta) \\
\end{aligned}
$$

Each $N_t$ follows a **Poisson distribution with rate** $m(t)$, where

$$
m(t) = \int_0^t \lambda(s) ds
$$

The stationarity also changes. We have

$$
N_t-N_s \sim \textrm{Poisson}(\int_s^t \lambda(u) du)=\textrm{Poisson}(m(t)-m(s))
$$  

### Deriving the forward equations  

An important technique for deriving concrete probability mass functions using the single arrival property.  

$$
\begin{aligned}
  p_n(t+\delta) = \Pr(N_{t+\delta}=n) &= \sum_{k=0}^n \Pr(N_{t+\delta}=n \mid N_t=k) \Pr(N_t=k) \\
                                      &= \sum_{k=0}^n \Pr(N_{t+\delta}-N_t=n-k \mid N_t=k) \Pr(N_t=k) \\
                                      &= \sum_{k=0}^n \Pr(N_{t+\delta}-N_t=n-k) \Pr(N_t=k) \\
                                      &= (1-\lambda (t) \delta)p_n(t) + \lambda (t) \delta p_{n-1}(t) + o(\delta) \\
\end{aligned}
$$  

Note the use of independence of increments and the single arrival property.  

This gives the differential equation  

$$
\frac{dp_n(t)}{dt} = \lambda (t) p_{n-1}(t) - \lambda (t) p_n(t)
$$  

When $n=0$,  

$$
\frac{dp_0(t)}{dt} = -\lambda (t) p_0(t)
$$


## Compound Poisson processes

Let $\{N_t\}_{t\geq 0}$ be a Poisson process with rate $\lambda>0$ and $\{Y_n\}_n$ be a sequence of identically, indepdently distributed random variables that are also *independent* of $\{N_t\}_{t\geq 0}$.

$$
S_t = \sum_{n=1}^{N_t} Y_n
$$

is a **compound Poisson process**.

The mean and variance of $S_t$ are

$$
\begin{aligned}
  \mathbb{E}[S_t] &= \lambda t \ \mathbb{E}[Y_1] \\
  \textrm{Var}[S_t] &= \lambda t \ \mathbb{E}[Y_1^2] 
\end{aligned}
$$

This is proveb by conditioning on $N_t$ and using the fact that $Y_n$ are independent.

We also recall the laws of total expectation and total variance.

$$
\begin{aligned}
  \mathbb{E}[S_t] &= \mathbb{E}[\mathbb{E}[S_t \mid N_t]] \\
  \textrm{Var}[S_t] &= \mathbb{E}[\textrm{Var}[S_t \mid N_t]] + \textrm{Var}[\mathbb{E}[S_t \mid N_t]]
\end{aligned}
$$

## Cramer-Lundberg

An application of the compound Poisson process is the Cramer-Lundberg model.

For an insurance company, there are **claims** $S_t$ (expense to pay when there are accidents) modelled by a **compound Poisson process**, **initial capital** $u$, and **premiums** $ct$ (money collected from customers with rate $c$).

We define the **risk process** to be

$$
U_t = u + ct - S_t, \qquad t \geq 0
$$

The company goes bankrupt if $U_t < 0$.

Thus, the **ruin probability** is defined as

$$
\psi(u, T) = \Pr(U_T <0 \text{\ for \ some \ t} \leq T), \qquad T > 0, u \geq 0
$$

The **total claim amount** $\{S_t\}_{t\geq 0}$ is

$$
S_t = \begin{cases}
  \sum_{n=1}^{N_t} Y_n & \textrm{if $N_t \geq 1$} \\
  0 & \textrm{otherwise}
\end{cases}
$$

where $N_t$ is a Poisson process with rate $\lambda$ and $Y_n$ are independent and identically distributed random variables with finite mean $\mu$ and variance $\sigma^2$.

We can compute the expected value of the risk process.

$$
\mathbb{E}[U_t] = u + ct - \lambda t \mu
$$

Therefore, a minimal requirement for this company to choose premium rate could be

$$
c > \lambda \mu
$$

this is called the **net profit condition**.

## Coalescent Process

The coalescent process describes the merging of $n$ offspring into a single ancestor occuring at random times.

-   We have $n$ individuals at time $t=0$

-   Each pair of individuals merge according to a Poisson process with rate $\lambda=1$ and there are $\binom{n}{2}$ pairs

-   The time of first coalescence follows $\text{Exp}(\binom{n}{2})$ distribution

-   There are $n-1$ coalescences

-   The process is in fact a death process

We can compute the time to the most recent common ancestor (i.e. the time of the last coalescence).

$$
\mathbb{E}\left(\sum_{k=1}^{n-1} H_k \right) \qquad n \in \mathbb{N}, n \geq 2
$$

with

$$
H_k \sim \text{Exp}(\binom{n-(k-1)}{2})
$$

So it follows that

$$
\mathbb{E}\left(\sum_{k=1}^{n-1} H_k \right) = \sum_{k=1}^{n-1} \frac{2}{k(k+1)}=2(1-\frac{1}{n})
$$

Comparing with the last coalescence time, we have  

$$
\mathbb{E}(H_{n-1})=1>2(1-\frac{1}{n})
$$  

showing that the last coalescence time is larger than half of the expected total coalescence time.

# Continuous-time Markov chains

A continuous-time process $\{X_t\}_{t \in [0, \infty)}$ satisfies the **Markov property** if

$$
\Pr(X_{t_n}=j \mid X_{t_1}=i_1, \ldots, X_{t_{n-1}}=i_{n-1}) = \Pr(X_{t_n}=j \mid X_{t_{n-1}}=i_{n-1})
$$

for all $j, i_1, \ldots, i_{n-1} \in E$ and for any sequence $0 \leq t_1 < \cdots < t_n < \infty$.

The **transition probability** is $p_{ij}(s,t)$, for $s \leq t$, $i, j \in E$  

$$
p_{ij}(s,t) = \Pr(X_t=j \mid X_s=i)
$$

The chain is **homogeneous** if   

$$
p_{ij}(s,t) = p_{ij}(0,t-s)
$$  

In this course, it is always assumed that the chain is homogeneous.  

**Theorem** The family is a **stochastic semigroup** if:

-   $\mathbf{P}_0 = I_{K \times K}$

-   $\mathbf{P}_t$ is stochastic

-   Chapman-Kolmogorov equations are satisfied  

$$
p_{ij}(s+t) = \sum_{k\in E} p_{ik}(s) p_{kj}(t)
$$

The semigroup $\{P_t\}$ is called **standard** if

$$
\lim_{t \downarrow 0} \mathbf{P}_t = \mathbf{I}
$$

*The Poisson process is a continuous time Markov chain.*

## Holding times

We define the **holding time at state i** as

$$
H_{|i} = \inf \{s \geq 0: X_{t+s} \neq i\}
$$

**Theorem** The holding time follows an **exponential distribution** (due to its memoryless property)

### Exponential Alarm Clocks

-   For each state $i \in E$, it can reach $n_i$ states

-   Set $n_i$ independent exponential alarm clocks with rates $q_{ij}$

-   The state transfers to the index of the first alarm clock that rings

-   Transfer to state $j$ with probability $\dfrac{q_{ij}}{\sum_k q_{ik}}$ (ordering of exponential random variables)

## The generator

The **generator** $\mathbf{G}=(g_{ij})_{i,j\in E}$ of the Markov chain with stochastic semigroup $P_t$ is defined as the $\rm{card}(E) \times \rm{card}(E)$ matrix

$$
\mathbf{G} = \lim_{\delta \downarrow 0} \dfrac{1}{\delta} [\mathbf{P_{\delta}}-\mathbf{I}]
$$

Hence we have the estimates for transition probabilities

$$
p_{ij}(\delta) \approx g_{ij} \delta = q_{ij} \delta \\
p_{ii}(\delta) \approx 1 + g_{ii} \delta = -\sum_{j \in E} q_{ij} \delta
$$

## Forward and backward equations

**Theorem** Subject to regularity conditions, a continuous-time Markov chain with stochastic semigroup $\{P_t\}$ and generator $\mathbf{G}$ satisfies the **Kolmogorov forward equation** and the **Kolmogorov backward equation**

$$
\begin{aligned}
  \mathbf{P}_t' &= \mathbf{P}_t \mathbf{G} \\
\mathbf{P}_t' &= \mathbf{G} \mathbf{P}_t 
\end{aligned}
$$

This allows us to write

$$
\mathbf{P}_t = \exp(t\mathbf{G})
$$

using matrix exponential.

## Irreducibility and stationarity

The chain is **irreducible** if for all $i,j \in E$ there exists $t>0$ such that $p_{ij}(t)>0$.

A distribution is the **stationary distribution** if it satisfies

$$
\mathbf{\pi} \mathbf{P}_t = \mathbf{\pi}
$$

for all $t \geq 0$.

A distribution $\pi$ is the **limiting distribution** if for all $i,j \in E$

$$
\lim_{t \to \infty} p_{ij}(t) = \pi_j
$$

**Theorem (find stationary distr)**\
Subject to regularity conditions, $\pi=\pi \mathbf{P}_t$ for all $t \geq 0$ if and only if $\pi \mathbf{G}=0$,.

**Theorem (Ergodicity in continuous time)**

1.  If there exists a stationary distribution, then it is *unique* and $\forall i,j \in E$
    $$
    \lim_{t\to +\infty} p_{ij}(t) = \pi_j
    $$

2.  If there is no stationary distribution then 
$$
    \lim_{t \to +\infty} p_{ij}(t) = 0
$$

## Jump chain and explosion

### From continuous to discrete

-   $J_n$ being the $n^{th}$ change in value of the chain $X$, $J_0=0$

-   Values right after the jump $Z_n = X_{J_n+}$ form a discrete time Markov chain

-   Construct transition matrix $p_{ij}^Z=\dfrac{g_{ij}}{-g_{ii}}$ and $0$ if absorption (all the diagonal entries are $0$)

-   $\{Z_n\}_{n\geq 0}$ is the **jump chain**

### From discrete to continuous

-   Let $p_{ii}^Z=0$ to avoid jumps to itself in the discrete chain

-   Construct generator matrix with arbitrary nonnegative $g_i$ for each $i$

$$
g_{ij} = \begin{cases}
g_ip_{ij}^Z & i\neq j \\
-g_i & i=j
\end{cases}
$$

-   Condition on $Z_i$, let $H_i \sim \rm{Exp}(g_{Z_{i-1}})$ be the 'holding times'

-   Then at time $t$, check if between two jump times

$$
X_t = \begin{cases}
Z_n & J_n \leq t < J_{n+1} \\
\infty & \rm{otherwise}
\end{cases}
$$

The chain explodes if $\Pr(J_{\infty} < \infty)>0$.

## Relation between common quantities

````{=html}
<!-- ```{=html}
<table>
<caption>Different 'transition matrices' in continuous time</caption>

Notation           Element							                      Meaning and Condition
---------------    --------------------------------------     -------------------------------------------------
$q_{ij}$           $q_{i}=\sum_{j \in E} q_{ij}$		          $q_{ii}=0$ and $q_{ij}>0$ the exponential rates  
---------------    --------------------------------------     -------------------------------------------------
$\mathbf{G}$       $g_{ij}=q_{ij}$ and $g_{ii}=-q_{ii}$       generator, $\mathbf{P}_t=\exp(tG)$
---------------    --------------------------------------     -------------------------------------------------
$\mathbf{P}_t$     $p_{ij}(t)=\exp(tG)_{ij}$                  the stochastic semigroup, a stochastic matrix
---------------    --------------------------------------     -------------------------------------------------
$\mathbf{P}^Z$     $p_{ij}=g_{ij}/g_i$		                    transition matrix of jump chain, stochastic  
---------------    --------------------------------------     -------------------------------------------------
</table>
``` -->
````

```{=tex}
\setlength{\extrarowheight}{.5em}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
Notation & Element & Meaning and Conditions  \\
\hline
$q_{ij}$ & $q_{i}=\sum_{j \in E} q_{ij}$ &   The \textbf{exponential rates} $q_{ij}>0$ when $i \neq j$ and $i \leftrightarrow j$, zero otherwise  \\

$\mathbf{G}$   &  $g_{ij}=q_{ij}$ and $g_{ii}=-q_{ii}$  & \textbf{generator}, $\mathbf{P}_t=\exp(t\mathbf{G})$, not stochastic, row sum is $0$ \\

$\mathbf{P}_t$  & $p_{ij}(t)=\exp(tG)_{ij}$ &  the \textbf{stochastic semigroup}, transition matrix at time $t$, a stochastic matrix \\

$\mathbf{P}^Z$ & $p_{ij}^Z=-g_{ij}/g_{ii}=q_{ij}/q_i$    & transition matrix of \textbf{jump chain}, a stochastic matrix\\
\hline
\end{tabular}
\end{table}
```
## Birth Processes

A **birth process** with intensities $\lambda_1, \lambda_2, \ldots$ is a continuous Markov chain $\{N_t\}_{t\geq 0}$ with nonnegative values such that

-   It is non-decreasing

-   There is 'single arrival'

$$
\Pr(N_{t+\delta}=n+m \mid N_t = n) = \begin{cases}
1- \lambda_n \delta + o(\delta) & m=0 \\
\lambda_n\delta + o(\delta) & m=1 \\
o(\delta) & m>1
\end{cases}
$$

-   Conditional on $N_s$, the increment $N_t-N_s$ is independent of all arrivals prior to time $s$, where $t>s$.

A birth process with constant intensity is a Poisson process. (Poisson process is a special case of birth process.)

It has generator $\mathbf{G}$

$$
\mathbf{G} = \begin{pmatrix}
-\lambda_0 & \lambda_0 & 0 & \cdots & 0 & \cdots \\
0 & -\lambda_1 & \lambda_1 & 0 & 0 & \cdots \\
0 & 0 & -\lambda_2 & \lambda_2 & 0  & \cdots \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
\end{pmatrix}
$$

### Simple Birth Process

-   We take intensities $\lambda_n = n \lambda$

$$
\Pr(N_{t+\delta}=n+m \mid N_t = n) = \binom{n}{m} (\lambda \delta)^m (1-\lambda \delta)^{n-m} + o(\delta)
$$

which gives

$$
\Pr(N_{t+\delta}=n+m \mid N_t = n) = \begin{cases}
  (1-\lambda \delta)^n + o(\delta) & m=0 \\
  n \lambda \delta (1-\lambda \delta)^{n-1} + o(\delta) & m=1 \\
  o(\delta) & m>1
\end{cases} 
$$  
Note that the higher order terms are $o(\delta)$, so we have $1-n\lambda \delta + o(\delta)$ and $n\lambda \delta + o(\delta)$.  

The **Forward & Backward** equations are given by

$$
\begin{aligned}
p'_{ij}(t) &= -\lambda_j p_{ij}(t) + \lambda_{j-1} p_{i, j-1}(t)  \\
p'_{ij}(t) &= -\lambda_i p_{ij}(t) + \lambda_{i} p_{i+1, j}(t)
\end{aligned}
$$  

### Deriving the Forward & Backward Equations  

We need to use the Chapman-Kolmogorov equations  

$$
p_{ij}(t+\delta) = \sum_{l\in E} p_{il}(t) p_{lj}(\delta)
$$  

which gives the forward direction with $p_{i,j-1}(t)\lambda_{j-1} \delta + p_{ij}(t) (1-\lambda_j \delta) + o(\delta)$.  

The backward direction is similar but 'splitting' in a different way.  

$$
p_{ij}(t+\delta) = \sum_{l\in E} p_{il}(\delta) p_{lj}(t)
$$  

with $p_{i+1,j}(t) \lambda_i \delta + p_{ij}(t) (1-\lambda_i \delta) + o(\delta)$.  

**Theorem** The forward equation has a unique solution, which is also satisfied by the backward equation.

## Birth-Death Processes

The **birth-death process** $\{X_t\}_{t\geq 0}$ is a continuous-time Markov chain taking values in $\mathbb{N}_0$ such that

-   The birth rates $\lambda_n$ and death rates $\mu_n$ are nonnegative with $\mu_0=0$

-   The infinitesimal transition probabilities are

$$
\Pr(X_{t+\delta}=n+m \mid X_t = n) = \begin{cases}
1- (\lambda_n + \mu_n)\delta + o(\delta) & m=0 \\
\lambda_n\delta + o(\delta) & m=1 \\
\mu_n\delta + o(\delta) & m=-1 \\
o(\delta) & |m|>1
\end{cases}
$$  

*The single arrival property rids us of the cancellation of birth and death.*  


The **stationary distribution** of a birth-death process is

$$
\mathbf{\pi}_n = \dfrac{\lambda_0 \times \cdots \lambda_{n-1}}{\mu_1 \times \cdots \times \mu_n} \pi_0
$$

with normalizing constant when the sum $\sum_{n=0}^\infty \mathbf{\pi}_n < \infty$

$$
\pi_0 = \dfrac{1}{\sum_{n=0}^\infty \mathbf{\pi}_n}
$$

### Immigration

# Brownian Motions

A real-valued stochastic process $B=\{B_t\}_{t\geq 0}$ is a **Brownian Motion** if

-   $B_0=0$ almost surely

-   $B$ has independent increments

-   $B$ has stationary increments

-   The increments are Gaussian, for $0 \leq s < t$

$$
B_t - B_s \sim N(0, t-s)
$$

-   The samples paths are a.s. continuous. ($t \mapsto B_t$ is a.s. continuous)  

A Brownian motion with **drift** $\mu$ and **variance** $\sigma^2$ is given by  

$$
Y_t = \mu t + \sigma B_t
$$  

then we have  

$$
Y_t-Y_s \sim N(\mu (t-s), \sigma^2 (t-s))
$$

## Construction of Brownian Motion

Consider the random walk $X_n = \sum_{i=1}^n Y_n$ with $Y_i \in \{-1,1\}$, from the central limit theorem, we have

$$
\dfrac{X_n}{\sqrt{n}} \overset{d}{\to} N(0, 1)
$$

We define the Brownian motion as a limit when $n \to \infty$

$$
B^{(n)}_t = \dfrac{X_{\lfloor nt \rfloor}}{\sqrt{n}} \overset{d}{\to} N(0, t)
$$

by Slutsky's Theorem. So $\dfrac{X_{\lfloor nt \rfloor}}{\sqrt{n}} \overset{d}{\to} B_t$

## Properties  

The covariance of $B_t$ and $B_s$ is  

$$
\text{Cov}(B_t, B_s) = \min(t,s)
$$  

### The symmetries of Brownian motion

Let $B_t$ be a standard Brownian motion, then each of the following is also a Brownian motion:  

- (Reflection) $\{-B_t\}$ 

- (Translation) $\{B_{t+s} - B_s\}$ 

- (Rescaling) For $a>0$, $\{aB_{t/a^2}\}$ 

- (Inversion)  $\{tB_{1/t}\}$   

### Reflection  

The **stopping-time** $\tau$ is the first time $B_t$ hits $x$ for some $x>0$.  

$$
\tau = \inf \{t \geq 0 \mid B_t \geq x\}
$$

The **reflected Brownian motion** $B''_t$ is given by  

$$
B''_t = \begin{cases}
B_t & t \leq \tau \\
x-(B_t-x) & t > \tau
\end{cases}
$$  

This is also a Brownian motion.  

The **maximum and minimum processes** of a Brownian motion are given by  

$$
\begin{aligned}
  M_t^+ &= \max_{0 \leq s \leq t} B_s \\
  M_t^- &= \min_{0 \leq s \leq t} B_s
\end{aligned}
$$  

The distribution of $M_t^+$ is given by  

$$
\Pr(M_t^+ \geq x) = \Pr(\tau \leq t) =2-2\Phi(x/\sqrt{t})
$$
whence the density of $\tau$ is given by  

$$
p_{\tau}(t) = \frac{x}{\sqrt{2\pi t^3}} \exp(-\frac{x^2}{2t})
$$