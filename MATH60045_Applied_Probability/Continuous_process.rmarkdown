---
title: "Continuous Time Stochastic Processes"
date:  '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  pdf_document:
    number_sections: true
header-includes:  
  \usepackage{dcolumn}
  \usepackage{amsmath}
  \usepackage{tabularx}
  \usepackage{array}
---

```{r echo=FALSE}
library(knitr)
library(kableExtra)
```

# Preliminaries

## Types of Processes

A **right continuous** stochastic process.

There are three types of right continuous processes

-   **Normal**

-   **Absorption**

-   **Explosion**

The **jump times** are random variables

The **holding times** are random variables defined as

A **jump process**

Compute probabilities using **countable union**

A **counting process** is a stochastic process $\{N_t\}_{t\geq 0}$ satisfying

-   $N_0=0$

-   $\forall t \geq 0, N_t \in \mathbb{N}_0$

-   (Non-decreasing) If $0 \leq s \leq t$, $N_s \leq N_t$

-   (Counting) When $s<t$, $N_t-N_s$ equals the no. of events in $(s,t]$

-   (Right continuous) The process is piecewise constant and has upward jumps (single step) of size $1$, therefore

$$
N_{t^-} = \lim_{s \uparrow t} N_s
$$

A **counting process associated the sequence** $(J_n)_{n \in \mathbb{N}_0}$

## Properties of random variables

The **exponential random variable** has

$$
f_X(x)=\lambda e^{-\lambda x}
$$

and c.d.f.

$$
F_X(x)=1-e^{-\lambda x}
$$

with a nonnegative support.

It has expectation

$$
\mathbb{E}[X]=\dfrac{1}{\lambda}
$$

and variance

$$
\textrm{Var}[X]=\dfrac{1}{\lambda^2}
$$

The **memoryless property** of a random variable refers to the fact:

$$
\Pr(X>x+y \mid X >x) = \Pr(X>y)
$$

-   A continuous random variable is memoryless iff it is $\textrm{Exp}(\lambda)$

-   A discrete random variable is memoryless iff it is $\textrm{Geom}(p)$

The **sum of exponential** $\textrm{Exp}(\lambda)$ is a $\textrm{Gamma}(n,\lambda)$ distribution

$$
f_{J_n}(t)=\dfrac{\lambda^n}{\Gamma(n)} t^{n-1} e^{-\lambda t}, \qquad t>0
$$

The **convergence for infinite sum of exponential** has the following criteria

-   If $\sum \frac{1}{\lambda_i} < \infty$, then $\Pr(J_{\infty}<\infty)<1$

-   If $\sum \frac{1}{\lambda_i}=\infty$, then $\Pr(J_{\infty}=\infty)=1$

The **minimum of exponential** is

$$
H \sim \textrm{Exp}(\sum_{i=1}^n \lambda_i)
$$

and the probability of any of the $k$ variables being the minimum is

$$
\Pr(H=H_k)=\dfrac{\lambda_k}{\sum_{i=1}^n \lambda_i}
$$

The **Laplace Transform** of a random variable $X$ is given by

$$
\mathcal{L}_X(u) = \mathbb{E}[e^{-uX}]
$$

A list of transformations for common random variables:

-   (Poisson) $\exp(\lambda t [e^{-u}-1])$

-   (Exponential) $\dfrac{\lambda}{\lambda+u}$

The **characteristic function** of a random variable $X$ is given by

$$
\phi_X(t) = \mathbb{E}[e^{itX}]
$$

# Poisson Processes

## Definitions

A **Poisson process**, denoted $\{N_t\}_{t\geq 0}$, is a non-decreasing stochastic process with nonnegative values satisfying

-   $N_0=0$

-   The increments are independent, $0 \leq t_0 \leq t_1 \leq \ldots \leq t_n$, the random variables $N_{t_0}, N_{t_1}-N_{t_0}, \ldots, N_{t_n}-N_{t_{n-1}}$ are independent

-   The increments are stationary

$$
\Pr(N_t-N_s=k)=\Pr(N_{t-s}=k)
$$

-   There is a single arrival (only one arrives in a small interval), for all $t \geq 0$ and $\delta >0$, $\delta \to 0$

$$
\begin{aligned}
    \Pr(N_{t+\delta}-N_t=1) &= \lambda \delta + o(\delta) \\
    \Pr(N_{t+\delta}-N_t \geq 2) &= o(\delta) \\
    \Pr(N_{t+\delta}-N_t = 0) &= 1- \lambda \delta + o(\delta)
\end{aligned}
$$

An **equivalent definition** replaces the last condition with the variable being Poisson with rate $N_t$

$$
  \Pr(N_t=k) = \dfrac{(\lambda t)^k}{k!} e^{-\lambda t}
$$

Another **equivalent definition** characterizes Poisson process $\{N_t\}_{t\geq 0}$ explicitly

-   Let $H_1, H_2, \ldots$ denote i.i.d. $\textrm{Exp}(\lambda)$ random variables

-   Let $J_0=0$ and $J_n = \sum_{i=1}^n H_i$

-   We define

$$
N_t = \sup \{n \in \mathbb{N}_0: J_n \leq t \}
$$

## Properties of Poisson Process

### Inter-arrival times

The inter-arrival times are **i.i.d.** $\textrm{Exp}(\lambda)$ random variables

### Time to $n^{th}$ event

The time to $n^{th}$ event is defined as

$$
J_n = \sum_{i=1}^n H_i
$$

which follows a $\textrm{Gamma}(n,\lambda)$ distribution

$$
f_{J_n}(t)=\dfrac{\lambda^n}{\Gamma(n)} t^{n-1} e^{-\lambda t}, \qquad t>0
$$

### Conditional distribution of arrival times

The conditional joint density of $(J_1, \ldots, J_n)$ is given by the order statistic

$$
f_{(J_1, \ldots, J_n)}(t_1, \ldots, t_n \mid N_t=n)=\begin{cases}
  \dfrac{n!}{t^n} & 0<t_1<\ldots<t_n \\
  0 & \textrm{otherwise}
\end{cases}
$$

The expectation of the $k^{th}$ value of $n$ uniformly distributed order statistics on $[0,t]$ is\
$$
\mathbb{E}[X_{(k)}]=\dfrac{tk}{n+1}=\mathbb{E}[J_k \mid N_t=n]
$$

## Extensions to Poisson Processes

### Superposition

Given $n$ independent Poisson processes, their sum is also a Poisson process

This is called a **superposition of Poisson processes**.

### Thinning

Each arrival of a Poisson Process is marked as a type $k$ event with probability $p_k$.

## Non-homogenous Poisson processes

## Compound Poisson processes

### Cramer-Lundberg

## Coalescent Process

# Continuous-time Markov chains

A continuous-time process $\{X_t\}_{t \in [0, \infty)}$ satisfies the **Markov property** if

$$
\Pr(X_{t_n}=j \mid X_{t_1}=i_1, \ldots, X_{t_{n-1}}=i_{n-1}) = \Pr(X_{t_n}=j \mid X_{t_{n-1}}=i_{n-1})
$$

for all $j, i_1, \ldots, i_{n-1} \in E$ and for any sequence $0 \leq t_1 < \cdots < t_n < \infty$.

The **transition probability**

The chain is **homogeneous**

**Theorem** The family is a **stochastic semigroup** if:

-   $\mathbf{P}_0 = I_{K \times K}$

-   $\mathbf{P}_t$ is stochastic

-   Chapman-Kolmogorov

The semigroup $\{P_t\}$ is called **standard** if

$$
\lim_{t \downarrow 0} \mathbf{P}_t = \mathbf{I}
$$

*The Poisson process is a continuous time Markov chain.*

## Holding times

We define the **holding time at state i** as

$$
H_{|i} = \inf \{s \geq 0: X_{t+s} \neq i\}
$$

**Theorem** The holding time follows an **exponential distribution** (due to its memoryless property)

### Exponential Alarm Clocks

## The jump chain

## The generator

## Forward and backward equations

## Irreducibility and stationarity

The chain is **irreducible**

A distribution is the **stationary distribution**

**Theorem (find stationary distr)**\
Subject to regularity conditions, $\pi=\pi \mathbf{P}_t$ for all $t \geq 0$ if and only if $\pi \mathbf{G}=0$,.

A distribution $\pi$ is the **limiting distribution** if for all $i,j \in E$

$$
\lim_{t \to \infty} p_{ij}(t) = \pi_j
$$

**Theorem (Ergodicity in continuous time)**

1.  If there exists a stationary distribution, then it is *unique* and $\forall i,j \in E$\
    $$
    \lim_{t\to +\infty} p_{ij}(t) = \pi_j
    $$

2.  If there is no stationary distribution then\
    $$
    \lim_{t \to +\infty} p_{ij}(t) = 0
    $$

## Jump chain and explosion

### From continuous to discrete

-   $J_n$ being the $n^{th}$ change in value of the chain $X$, $J_0=0$

-   Values right after the jump $Z_n = X_{J_n+}$ form a discrete time Markov chain

-   Construct transition matrix $p_{ij}^Z=\dfrac{g_{ij}}{-g_{ii}}$ and $0$ if absorption (all the diagonal entries are $0$)

-   $\{Z_n\}_{n\geq 0}$ is the **jump chain**

### From discrete to continuous

-   Let $p_{ii}^Z=0$ to avoid jumps to itself in the discrete chain

-   Construct generator matrix with arbitrary nonnegative $g_i$ for each $i$

$$
g_{ij} = \begin{cases}
g_ip_{ij}^Z & i\neq j \\
-g_i & i=j
\end{cases}
$$

-   Condition on $Z_i$, let $H_i \sim \rm{Exp}(g_{Z_{i-1}})$ be the 'holding times'

-   Then at time $t$, check if between two jump times

$$
X_t = \begin{cases}
Z_n & J_n \leq t < J_{n+1} \\
\infty & \rm{otherwise}
\end{cases}
$$

The chain explodes if $\Pr(J_{\infty} < \infty)>0$.

## Relation between common quantities

````{=html}
<!-- ```{=html}
<table>
<caption>Different 'transition matrices' in continuous time</caption>

Notation           Element							                      Meaning and Condition
---------------    --------------------------------------     -------------------------------------------------
$q_{ij}$           $q_{i}=\sum_{j \in E} q_{ij}$		          $q_{ii}=0$ and $q_{ij}>0$ the exponential rates  
---------------    --------------------------------------     -------------------------------------------------
$\mathbf{G}$       $g_{ij}=q_{ij}$ and $g_{ii}=-q_{ii}$       generator, $\mathbf{P}_t=\exp(tG)$
---------------    --------------------------------------     -------------------------------------------------
$\mathbf{P}_t$     $p_{ij}(t)=\exp(tG)_{ij}$                  the stochastic semigroup, a stochastic matrix
---------------    --------------------------------------     -------------------------------------------------
$\mathbf{P}^Z$     $p_{ij}=g_{ij}/g_i$		                    transition matrix of jump chain, stochastic  
---------------    --------------------------------------     -------------------------------------------------
</table>
``` -->
````

```{=tex}
\setlength{\extrarowheight}{.5em}
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
Notation & Element & Meaning and Conditions  \\
\hline
$q_{ij}$ & $q_{i}=\sum_{j \in E} q_{ij}$ &   The \textbf{exponential rates} $q_{ij}>0$ when $i \neq j$ and $i \leftrightarrow j$, zero otherwise  \\

$\mathbf{G}$   &  $g_{ij}=q_{ij}$ and $g_{ii}=-q_{ii}$  & \textbf{generator}, $\mathbf{P}_t=\exp(t\mathbf{G})$, not stochastic, row sum is $0$ \\

$\mathbf{P}_t$  & $p_{ij}(t)=\exp(tG)_{ij}$ &  the \textbf{stochastic semigroup}, transition matrix at time $t$, a stochastic matrix \\

$\mathbf{P}^Z$ & $p_{ij}^Z=-g_{ij}/g_{ii}=q_{ij}/q_i$    & transition matrix of \textbf{jump chain}, a stochastic matrix\\
\hline
\end{tabular}
\end{table}
```
## Birth Processes

A **birth process** with intensities $\lambda_1, \lambda_2, \ldots$ is a continuous Markov chain $\{N_t\}_{t\geq 0}$ with nonnegative values such that

-   It is non-decreasing

-   There is 'single arrival'

$$
\Pr(N_{t+\delta}=n+m \mid N_t = n) = \begin{cases}
1- \lambda_n \delta + o(\delta) & m=0 \\
\lambda_n\delta + o(\delta) & m=1 \\
o(\delta) & m>1
\end{cases}
$$

-   Conditional on $N_s$, the increment $N_t-N_s$ is independent of all arrivals prior to time $s$, where $t>s$.

A birth process with constant intensity is a Poisson process.

### Simple Birth Process

The **Forward & Backward** equations are given by

The equations have a unique solution

## Birth-Death Processes

### Stationary Distributions

The solution to $\pi G = 0$ is given by

### Immigration

# Brownian Motions

A real-valued stochastic process $B=\{B_t\}_{t\geq 0}$ is a **Brownian Motion** if

-   $B_0=0$ almost surely

-   $B$ has independent increments

-   $B$ has stationary increments

-   The increments are Gaussian, for $0 \leq s < t$

$$
B_t - B_s \sim N(0, t-s)
$$

-   The samples paths are a.s. continuous. ($t \mapsto B_t$ is a.s. continuous)

## Construction of Brownian Motion

Consider the random walk $X_n = \sum_{i=1}^n Y_n$ with $Y_i \in \{-1,1\}$, from the central limit theorem, we have

$$
\dfrac{X_n}{\sqrt{n}} \overset{d}{\to} N(0, 1)
$$

We define the Brownian motion as a limit when $n \to \infty$

$$
B^{(n)}_t = \dfrac{X_{\lfloor nt \rfloor}}{\sqrt{n}} \overset{d}{\to} N(0, t)
$$

by Slutsky's Theorem. So $\dfrac{X_{\lfloor nt \rfloor}}{\sqrt{n}} \overset{d}{\to} B_t$

## Properties
