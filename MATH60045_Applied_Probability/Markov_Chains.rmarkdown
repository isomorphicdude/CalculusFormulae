---
title: "Markov Chains"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: 
  pdf_document:
    number_sections: true
  
---

# Basics  

## Chapman-Kolmogorov (CK) equations    

The **n-step** transition probability is  

$$
	p_{ij}(n) = \Pr(X_{m+n}=j | X_m=i)
$$

## First passage and hitting times   

The **first passage time** is   

$$
	T_j = \min \{n \in \mathbb{N}: X_n=j\}
$$   

In other words, $\{T_j=n\}=\{X_n=j, X_{n-1}\neq j, \cdots, X_1 \neq j\}$, if $X_n \neq j, \forall n \in \mathbb{N}$,  then $T_j=\infty$.

The **first passage probability** is   

$$
	f_{ij}(n) = \Pr(T_j=n|X_0=i), n\in \mathbb{N}_0
$$

from which the hitting probability follows  

$$
	f_{ij}=\Pr(T_j < \infty|X_0=i)=\sum_{n=0}^{\infty} f_{ij}(n)
$$  

With the special case being $f_{ij}(0)=0$.  

## Generating Functions of Markov Chain  

Recall the **probability generating function**  

$$
	G_X(s) = \sum_{x=0}^{\infty} s^x \Pr(X=x)
$$  

where this holds on the support

$$
	\mathcal{S_X} = \bigg \{s\in \mathbb{R}: \sum_{x=0}^{\infty} |s|^x \Pr(X=x) < \infty \bigg\}
$$  

The generating functions here are  

$$
\begin{aligned}
	G_{p_{ij}(n)} &= \sum_{n=0}^{\infty} p_{ij}(n) s^n \\
	G_{f_{ij}(n)} &= \sum_{n=0}^{\infty} f_{ij}(n) s^n
\end{aligned}
$$  

By arguing using equating coefficients and an identity, we have a **theorem**  

$$
	G_{p_{ij}(n)} = \delta_{ij} + G_{f_{ij}(n)}(s) G_{p_{ij}(n)}
$$  

The identity used is  

$$
	p_{ij}(n) = \sum_{l=1}^n f_{ij}(l)p_{jj}(n-l)
$$  



# Recurrence and Transience   

A state $j$ is **recurrent** if and only if  

$$
	\sum_{n=1}^{\infty} p_{jj}(n) = \infty
$$

A state $j$ is **transient** if and only if  
$$
	\sum_{n=1}^{\infty} p_{jj}(n) < \infty
$$  

**Examples**: Examples of [transient, irreducible chains](https://math.stackexchange.com/questions/242311/example-of-irreductible-transient-markov-chain)

The **number of periods** that the chain is in state $j$ (or **number of visits** to $j$) is  

$$
	N_j = \sum_{n=1}^{\infty} I_n(j)
$$  
where $I_n(j)$ is the indicator function taking value $1$ if $X_n=j$ and $0$ otherwise.  

The **expected number of visits** to state $j$ given $X_0=j$ is  

$$
	\mathbb{E}[N_j| X_0=j] = \sum_{n=0}^{\infty} p_{jj}(n)
$$


proof using generating functions:  

Taking $s\to 1$ and using Abel's theorem, we can deduce...


## Properties of recurrent/transient states  

**Theorem (Number of visits is geometric for transient states)**

If $j$ is transient, then  

$$
	\Pr(N_j=n | X_0=j) = f_{jj}^{n-1} (1-f_{jj}), n\in \mathbb{N}
$$  

Let $i\neq j$, then  

$$
	\Pr(N_j=n | X_0=i) = \begin{cases}
							1- f_{ij} & n=0 \\
							f_{ij} f_{jj}^{n-1} (1-f_{jj}) & n\geq 1
						\end{cases}
$$

Intuition is that the chain visits $j$ for the first time and returns to it for $n-1$ times, then leaves it.  

Therefore, it follows that  for $i\neq j$,  

$$
	\mathbb{E}[N_j| X_0=i] = \frac{f_{ij}}{1-f_{jj}}
$$  

and  

$$
	\mathbb{E}[N_j| X_0=j] = \frac{1}{1-f_{jj}}
$$  


**Theorem(Unlikely to visit a transient state)**

If $j$ is transient, then  

$$
	\lim_{n \to \infty} p_{ij}(n) = 0, \forall j \in E
$$  



## Mean recurrence time, null and positive recurrence  

The **mean recurrence time** $\mu_j$ is  

$$
	\mu_j=\mathbb{E}[T_j | X_0=j]=\sum_{n=1}^{\infty} n f_{jj}(n)
$$  
where we recall that $\{T_j=n\}=\{X_n=j, X_{n-1}\neq j, \cdots, X_1 \neq j\}$.  

Similarly, we can define the **mean first passage time**  

$$
	\mu_{ij}=\mathbb{E}[T_j | X_0=i]=\sum_{n=1}^{\infty} n f_{ij}(n)
$$

those expectations can be finite or infinite; for transient states, they must be infinite.  

**Theorem(mean first passage time)** 


For a recurrent state $j$, it is called **null recurrent** if $\mu_j=\infty$ and **positive recurrent** if $\mu_j<\infty$.  

**Theorem (unlikely to visit null recurrent state)** If $j$ is null recurrent, then  

$$
	\lim_{n \to \infty} p_{jj}(n) = 0, \forall j \in E
$$  

In addition,  
$$
	\lim_{n \to \infty} p_{ij}(n) = 0, \forall i \neq j \in E
$$

## Examples  

# Aperiodicity and Ergodicity  

The **period** of a state $j$ is  

$$
	d(j) = gcd\{n: p_{jj}(n)>0\}
$$  

It is not necessarily true that $p_{jj}(d(j))>0$ (cf. Notes Pg. 36).  

A state is **ergodic** if it is positive recurrent and aperiodic.  

# Communicating classes   

We say that a state $j$ is **accessible** from state $i$ if	the chain can reach $j$ at some time, written as $i\rightarrow j$.  
Two states $i$ and $j$ are **communicating** if there exists a state $k$ such that $i\rightarrow k$ and $k\rightarrow j$, we write $i\leftrightarrow j$; this is an **equivalence relation**.  
If $i \neq j$, then $i\rightarrow j$ if and only if $f_{ij}>0$.  


## Properties preserved by Communicating Classes  

- Same period
- Same transience/recurrence
- Null recurrence  

For a **set of states** $C$:  

- $C$ is **closed** if $\forall i \in C$, $j \notin C$, $p_{ij}=0$  

- $C$ is **irreducible** if all states in the set communicate with each other  

Therefore, an irreducible set of states share the same properties described above.  

**Theorem (Recurrence and closed)** If $C$ is a communicating class of recurrent states, then $C$ is closed.  

**Theorem (Stochastic matrix on closed states)**  The stochastic matrix $P$ restricted to a closed set of closed states $C$ is still a stochastic matrix.  

## Decomposition of Chains  

The state space can be partitioned into communicating classes.  

$$
	E = T \cup \left(\bigcup_i C_i\right)
$$  

where $T$ is the set of transient states and $C_i$'s are irreducible closed sets of recurrent states.  

## Class Properties  

The **classes** refer to communicating classes.

**Theorem (Finite Chains have recurrent)** When state space is **finite**, at least one state is *recurrent* and all *recurrent* states are **positive**  

**Remark** This combined with later results on stationarity makes a chain with finite state space particularly nice.  

**Remark** It follows that there are no null recurrent states in a finite state space.  

**Theorem (Finite and closed)** If $C$ is a finite, closed communicating class, then all states are positive recurrent.  

<table>
<caption>Communicating class properties</caption>

Type of Class     Finite							Infinite
-------------  -------------  				---------------------
Closed          positive recurrent			positive/null recurrent, transient
Not closed      transient					transient
</table>  


# Gambler's Ruin  

# Stationarity  

We are interested in the equilibrium states of a chain  

## Distribution  
- Distribution is a row vector $\lambda$ with $\Sigma_{j} \lambda_j=1$  
- If $\lambda P=\lambda$ then it is called *invariant*  
  
## Stationary distributions of irreducible chains  

**Theorem** Every irreducible chain has a **stationary distribution** $\pi$ if and only if all states are **positive recurrent** 
- $\pi$ is unique
- $\pi=\mu_i^{-1}$ the inverse of mean recurrence time    

We first have some lemmas:  
$$  
\begin{aligned}
l_{ji}(n)&=\Pr(X_n=i, T_j \geq n | X_0 =j) 
\end{aligned}
$$  
being the probability that the chain reaches $i$ in $n$ steps without returning to $j$    
**Lemma**
$$
	f_{jj}(m+n) = \sum_{i \in E, i\neq j} l_{ji}(m) f_{ij}(n)
$$  
from which $f_{jj}(m+n) \geq l_{ji}(m) f_{ij}(n)$ follows    

**Lemma** We also have the following recurrence relation

**Lemma**: A positive recurrent chain has a stationary distribution.


**Proof**: (constructive)  

- **(Step1 Construction)** Let $N_i(j)$ be the number of visits to state $i$ before state $j$ ; the sum of such numbers over i is equal to the hitting time $T_j$  
- Define $\rho_i(j)$ to be the expected number of visits to the state $i$ between two successive visits to state $j$   (in this step the **recurrence** of the chain is used, as the $T_j$ is finite with probability $1$)  
$$
\begin{aligned}
	\rho_i(j) &= \mathbb{E}[N_i(j) | X_0=j] \\
			  &= \sum_n \Pr(X_n=i, T_j\geq n | X_0=j) \\
			  &= \sum_n l_{ij}(n) 
\end{aligned}
$$

- Now the mean hitting time can be computed as 
$$
\begin{aligned}
	\mu_j &= \mathbb{E}\big[ \sum_i  N_i(j) | X_0=j\big] \\
		  &= \sum_i \rho_i(j)
\end{aligned}
$$  
- which can be written as sum of $\rho_i(j)$ by Tonelli and linearity of conditional expectation 

- **(Step2 	Finiteness)** Use a lemma to bound $\rho_i(j)$ so it's finite  
  
- Namely write $\rho_i(j)=\sum_n l_{ji} (n)$ and bound using the fact that the chain is irreducible, so there exists $f_{ij}(n^*)>0$, so $f_{jj}(m+n^*) \geq l_{ji}(m) f_{ij}(n^*)$
  
- **(Step3 Stationarity)** Use a recurrence to show     
$$
\begin{aligned}
\rho_i(j)   &=\sum_n l_{ji} (n) \\
				&= p_{ji} + \sum_{n=2} \sum_{r,r\neq j} p_{ri} l_{jr}(n-1)\\
				&=p_{ji}\rho_i(j) + \sum_{n=1} \sum_{r,r\neq j} p_{ri} l_{jr}(n)\\
				&=p_{ji}\rho_i(j) + \sum_{r,r\neq j} p_{ri}\sum_{n=1}  l_{jr}(n)\\
				&=\sum_r \rho_r(j)p_{ri}
\end{aligned} 
$$  

- This $\rho_i(j)$ does not necessarily give a probability vector when the chain is not positive recurrent.

- Now if the chain is positive recurrent, we have $\mu_j$ finite for every $j$, we have   
$$
	\pi_i = \frac{\rho_i(j)}{\mu_j}
$$

**Lemma** If a stationary distribution exists, then the chain is positive recurrent and the distribution must be given by $\pi_i = \mu_i^{-1}$  

**proof**:  ...





## Limiting Distribution  


## Ergodic Theorem  

## Summary of properties of irreducible chains   

# Time reversibility

